{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-20T06:21:16.369174Z",
     "start_time": "2025-10-20T06:21:13.056148Z"
    }
   },
   "source": [
    "# All imports go here in one place\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "# All stable-baselines3 imports\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Your custom environment\n",
    "from dino_env import DinoEnv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T06:31:29.229977Z",
     "start_time": "2025-10-20T06:21:16.378099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 1. CONFIGURE YOUR TRAINING SESSION ---\n",
    "SESSION_NAME = \"PPO_Optimized_Run1\"\n",
    "\n",
    "# --- 2. SETUP DIRECTORIES ---\n",
    "models_dir = f\"models/{SESSION_NAME}\"\n",
    "logdir = f\"logs/{SESSION_NAME}\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "# --- 3. AUTOMATICALLY DETECT LATEST CHECKPOINT ---\n",
    "latest_checkpoint = 0\n",
    "if os.path.exists(models_dir) and len(os.listdir(models_dir)) > 0:\n",
    "    # Find all saved model files and extract the timestep number\n",
    "    checkpoints = [int(f.split('.')[0]) for f in os.listdir(models_dir) if f.endswith('.zip')]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints)\n",
    "\n",
    "# --- 4. CREATE THE ENVIRONMENT ---\n",
    "env = make_vec_env(DinoEnv, n_envs=1)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# --- 5. DEFINE LEARNING RATE ---\n",
    "def linear_schedule(initial_value):\n",
    "    def func(progress_remaining):\n",
    "        return progress_remaining * initial_value\n",
    "    return func\n",
    "\n",
    "# --- 6. LOAD MODEL OR CREATE NEW ONE ---\n",
    "if latest_checkpoint > 0:\n",
    "    # --- RESUME TRAINING ---\n",
    "    MODEL_PATH = f\"{models_dir}/{latest_checkpoint}.zip\"\n",
    "    print(f\"✅ Resuming training from checkpoint: {MODEL_PATH}\")\n",
    "    model = PPO.load(MODEL_PATH, env=env, tensorboard_log=logdir)\n",
    "    # Optional: Reset learning rate to encourage new exploration\n",
    "    model.learning_rate = linear_schedule(0.00005)\n",
    "else:\n",
    "    # --- START FRESH TRAINING ---\n",
    "    print(f\"✅ Starting a new training session: {SESSION_NAME}\")\n",
    "    model = PPO(\n",
    "        'CnnPolicy', env, n_steps=4096, gamma=0.99,\n",
    "        learning_rate=linear_schedule(0.0001),\n",
    "        verbose=1, tensorboard_log=logdir\n",
    "    )\n",
    "\n",
    "# --- 7. START THE TRAINING LOOP ---\n",
    "TIMESTEPS_PER_INTERVAL = 20000\n",
    "TOTAL_INTERVALS = 50 # Let's aim for 1 million total steps\n",
    "\n",
    "# Calculate the starting interval based on our loaded checkpoint\n",
    "starting_interval = (latest_checkpoint // TIMESTEPS_PER_INTERVAL) + 1\n",
    "\n",
    "for i in range(starting_interval, TOTAL_INTERVALS + 1):\n",
    "    print(f\"--- Training Interval {i} of {TOTAL_INTERVALS} ---\")\n",
    "\n",
    "    model.learn(total_timesteps=TIMESTEPS_PER_INTERVAL, reset_num_timesteps=False, tb_log_name=SESSION_NAME)\n",
    "\n",
    "    save_path = f\"{models_dir}/{i * TIMESTEPS_PER_INTERVAL}\"\n",
    "    model.save(save_path)\n",
    "    print(f\"Checkpoint saved to {save_path}.zip\")\n",
    "\n",
    "print(\"--- Training complete! ---\")\n",
    "env.close()"
   ],
   "id": "692cb13aca4df256",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to locate the game window...\n",
      "Opening in existing browser session.\n",
      "✅ Found 'replay_button.png'.\n",
      "Game region calculated: {'left': 90, 'top': 235, 'width': 840, 'height': 200}\n",
      "Score region calculated: {'left': 690, 'top': 235, 'width': 235, 'height': 50}\n",
      "✅ Starting a new training session: PPO_Optimized_Run1\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "--- Training Interval 1 of 50 ---\n",
      "Logging to logs/PPO_Optimized_Run1/PPO_Optimized_Run1_0\n"
     ]
    },
    {
     "ename": "FailSafeException",
     "evalue": "PyAutoGUI fail-safe triggered from mouse moving to a corner of the screen. To disable this fail-safe, set pyautogui.FAILSAFE to False. DISABLING FAIL-SAFE IS NOT RECOMMENDED.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFailSafeException\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 58\u001B[39m\n\u001B[32m     55\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(starting_interval, TOTAL_INTERVALS + \u001B[32m1\u001B[39m):\n\u001B[32m     56\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m--- Training Interval \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTOTAL_INTERVALS\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m ---\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m58\u001B[39m     \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mTIMESTEPS_PER_INTERVAL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mSESSION_NAME\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     60\u001B[39m     save_path = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodels_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;250m \u001B[39m*\u001B[38;5;250m \u001B[39mTIMESTEPS_PER_INTERVAL\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     61\u001B[39m     model.save(save_path)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/DinoAI-RL/.venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:311\u001B[39m, in \u001B[36mPPO.learn\u001B[39m\u001B[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[39m\n\u001B[32m    302\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mlearn\u001B[39m(\n\u001B[32m    303\u001B[39m     \u001B[38;5;28mself\u001B[39m: SelfPPO,\n\u001B[32m    304\u001B[39m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    309\u001B[39m     progress_bar: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    310\u001B[39m ) -> SelfPPO:\n\u001B[32m--> \u001B[39m\u001B[32m311\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    313\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    314\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    315\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    317\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    318\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/DinoAI-RL/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001B[39m, in \u001B[36mOnPolicyAlgorithm.learn\u001B[39m\u001B[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[39m\n\u001B[32m    321\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.env \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    323\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m.num_timesteps < total_timesteps:\n\u001B[32m--> \u001B[39m\u001B[32m324\u001B[39m     continue_training = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[32m    327\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/DinoAI-RL/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001B[39m, in \u001B[36mOnPolicyAlgorithm.collect_rollouts\u001B[39m\u001B[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[39m\n\u001B[32m    213\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    214\u001B[39m         \u001B[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001B[39;00m\n\u001B[32m    215\u001B[39m         \u001B[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001B[39;00m\n\u001B[32m    216\u001B[39m         clipped_actions = np.clip(actions, \u001B[38;5;28mself\u001B[39m.action_space.low, \u001B[38;5;28mself\u001B[39m.action_space.high)\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m new_obs, rewards, dones, infos = \u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclipped_actions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    220\u001B[39m \u001B[38;5;28mself\u001B[39m.num_timesteps += env.num_envs\n\u001B[32m    222\u001B[39m \u001B[38;5;66;03m# Give access to local variables\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/DinoAI-RL/.venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001B[39m, in \u001B[36mVecEnv.step\u001B[39m\u001B[34m(self, actions)\u001B[39m\n\u001B[32m    215\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    216\u001B[39m \u001B[33;03mStep the environments with the given action\u001B[39;00m\n\u001B[32m    217\u001B[39m \n\u001B[32m    218\u001B[39m \u001B[33;03m:param actions: the action\u001B[39;00m\n\u001B[32m    219\u001B[39m \u001B[33;03m:return: observation, reward, done, information\u001B[39;00m\n\u001B[32m    220\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    221\u001B[39m \u001B[38;5;28mself\u001B[39m.step_async(actions)\n\u001B[32m--> \u001B[39m\u001B[32m222\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/DinoAI-RL/.venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:97\u001B[39m, in \u001B[36mVecTransposeImage.step_wait\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     96\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> VecEnvStepReturn:\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m     observations, rewards, dones, infos = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvenv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     99\u001B[39m     \u001B[38;5;66;03m# Transpose the terminal observations\u001B[39;00m\n\u001B[32m    100\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m idx, done \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dones):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/DinoAI-RL/.venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:39\u001B[39m, in \u001B[36mVecFrameStack.step_wait\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep_wait\u001B[39m(\n\u001B[32m     32\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m     33\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[\n\u001B[32m   (...)\u001B[39m\u001B[32m     37\u001B[39m     \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]],\n\u001B[32m     38\u001B[39m ]:\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m     observations, rewards, dones, infos = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvenv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     40\u001B[39m     observations, infos = \u001B[38;5;28mself\u001B[39m.stacked_obs.update(observations, dones, infos)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m     41\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m observations, rewards, dones, infos\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/DinoAI-RL/.venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001B[39m, in \u001B[36mDummyVecEnv.step_wait\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     56\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> VecEnvStepReturn:\n\u001B[32m     57\u001B[39m     \u001B[38;5;66;03m# Avoid circular imports\u001B[39;00m\n\u001B[32m     58\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m.num_envs):\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m         obs, \u001B[38;5;28mself\u001B[39m.buf_rews[env_idx], terminated, truncated, \u001B[38;5;28mself\u001B[39m.buf_infos[env_idx] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[assignment]\u001B[39;49;00m\n\u001B[32m     60\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mactions\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     61\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     62\u001B[39m         \u001B[38;5;66;03m# convert to SB3 VecEnv api\u001B[39;00m\n\u001B[32m     63\u001B[39m         \u001B[38;5;28mself\u001B[39m.buf_dones[env_idx] = terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/DinoAI-RL/.venv/lib/python3.12/site-packages/stable_baselines3/common/monitor.py:94\u001B[39m, in \u001B[36mMonitor.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m     92\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.needs_reset:\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mTried to step environment that needs reset\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m observation, reward, terminated, truncated, info = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[38;5;28mself\u001B[39m.rewards.append(\u001B[38;5;28mfloat\u001B[39m(reward))\n\u001B[32m     96\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/DinoAI-RL/dino_env.py:122\u001B[39m, in \u001B[36mDinoEnv.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    117\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m observation, reward, terminated, truncated, info\n\u001B[32m    119\u001B[39m \u001B[38;5;66;03m# --- If the game is NOT over, proceed with the action ---\u001B[39;00m\n\u001B[32m    120\u001B[39m \n\u001B[32m    121\u001B[39m \u001B[38;5;66;03m# Click to ensure the game has focus.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m \u001B[43mpyautogui\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgame_region\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mleft\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgame_region\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtop\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    124\u001B[39m reward = \u001B[32m0.01\u001B[39m  \u001B[38;5;66;03m# Base survival reward\u001B[39;00m\n\u001B[32m    126\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m action == \u001B[32m1\u001B[39m:  \u001B[38;5;66;03m# Jump\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/DinoAI-RL/.venv/lib/python3.12/site-packages/pyautogui/__init__.py:593\u001B[39m, in \u001B[36m_genericPyAutoGUIChecks.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    591\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(wrappedFunction)\n\u001B[32m    592\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapper\u001B[39m(*args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m593\u001B[39m     \u001B[43mfailSafeCheck\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    594\u001B[39m     returnVal = wrappedFunction(*args, **kwargs)\n\u001B[32m    595\u001B[39m     _handlePause(kwargs.get(\u001B[33m\"\u001B[39m\u001B[33m_pause\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/DinoAI-RL/.venv/lib/python3.12/site-packages/pyautogui/__init__.py:1734\u001B[39m, in \u001B[36mfailSafeCheck\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   1732\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfailSafeCheck\u001B[39m():\n\u001B[32m   1733\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m FAILSAFE \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(position()) \u001B[38;5;129;01min\u001B[39;00m FAILSAFE_POINTS:\n\u001B[32m-> \u001B[39m\u001B[32m1734\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m FailSafeException(\n\u001B[32m   1735\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mPyAutoGUI fail-safe triggered from mouse moving to a corner of the screen. To disable this fail-safe, set pyautogui.FAILSAFE to False. DISABLING FAIL-SAFE IS NOT RECOMMENDED.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1736\u001B[39m         )\n",
      "\u001B[31mFailSafeException\u001B[39m: PyAutoGUI fail-safe triggered from mouse moving to a corner of the screen. To disable this fail-safe, set pyautogui.FAILSAFE to False. DISABLING FAIL-SAFE IS NOT RECOMMENDED."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pyautogui\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# {'left': 100, 'top': 265, 'width': 650, 'height': 180}\n",
    "# --- Paste the coordinates from your log output here ---\n",
    "game_region = {'left': 80, 'top': 230, 'width': 840, 'height': 200}\n",
    "\n",
    "# --- Calculate the score region based on the game region ---\n",
    "score_region = {\n",
    "    'left': game_region['left'] + 600,\n",
    "    'top': game_region['top'],\n",
    "    'width': 235,\n",
    "    'height': 50\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Take a screenshot of the entire screen\n",
    "screenshot = pyautogui.screenshot()\n",
    "frame = np.array(screenshot)\n",
    "\n",
    "# Convert colors from RGB (pyautogui) to BGR (OpenCV)\n",
    "# <-- THE FIX IS HERE\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# --- Draw the GREEN box for the game region ---\n",
    "x1_game, y1_game = game_region['left'], game_region['top']\n",
    "x2_game, y2_game = game_region['left'] + game_region['width'], game_region['top'] + game_region['height']\n",
    "cv2.rectangle(frame, (x1_game, y1_game), (x2_game, y2_game), (0, 255, 0), 2) # Green\n",
    "\n",
    "# --- Draw the BLUE box for the score region ---\n",
    "x1_score, y1_score = score_region['left'], score_region['top']\n",
    "x2_score, y2_score = score_region['left'] + score_region['width'], score_region['top'] + score_region['height']\n",
    "cv2.rectangle(frame, (x1_score, y1_score), (x2_score, y2_score), (255, 0, 0), 2) # Blue\n",
    "\n",
    "# Display the result in a new window\n",
    "cv2.imshow(\"Region Test\", frame)\n",
    "\n",
    "print(\"A window named 'Region Test' has opened.\")\n",
    "print(\"The GREEN box is the AI's main vision.\")\n",
    "print(\"The BLUE box is where the AI reads the score.\")\n",
    "print(\"Press any key on that window to close it.\")\n",
    "\n",
    "# Wait for you to press a key, then close the window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "a6919f1262fdb2f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "m",
   "id": "d9e4f3e0dc182f6b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
